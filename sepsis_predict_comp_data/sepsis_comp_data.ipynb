{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sepsis_comp_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Co8xcuBnNQ-"
      },
      "source": [
        "# install env\n",
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda=9.0.176-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6d-mjTgnpVe"
      },
      "source": [
        "# test env\n",
        "import torch\n",
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoYL-BfrnuI_"
      },
      "source": [
        "# mount google drive to env\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/My Drive/colab/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcaqeZ5Bnx2b"
      },
      "source": [
        "# tcn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "print(\"TCN component initialized\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TCN component initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN7hQVSFn0X_"
      },
      "source": [
        "# model\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
        "        super(TCN, self).__init__()\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
        "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
        "        output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
        "        output = self.linear(output).float()\n",
        "        return self.sig(output)\n",
        "\n",
        "\n",
        "print(\"TCN model initialized\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TCN model initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeWmaLFGn-6w"
      },
      "source": [
        "# data_generator\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "nan_mode = 1 # method to manage NaN value\n",
        "data_dir = \"../data/training/\"\n",
        "\n",
        "\n",
        "def read_psv(f, X, Y):\n",
        "  lines = f.readlines()\n",
        "  m = np.zeros((len(lines)-1, 41), dtype=np.float32)\n",
        "  for i in range(1, len(lines)):\n",
        "    line = lines[i][:-1].split('|')\n",
        "    line = np.array(line, dtype=np.float32)\n",
        "    X[i-1] = line[:-1]\n",
        "    Y[i-1] = line[-1]\n",
        "\n",
        "\n",
        "def nan_process(m, alg=0):\n",
        "  if alg == 0:\n",
        "    # fill NaN value with 0 \n",
        "    m[np.isnan(m)] = 0\n",
        "  elif alg == 1:\n",
        "    # TODO: fill NaN value with average\n",
        "    pass\n",
        "  return m\n",
        "\n",
        "\n",
        "def data_generator(dataset='B', nan_alg=0):\n",
        "  '''\n",
        "  读入数据并分割为训练集、验证集、测试集，默认使用setB\n",
        "  \n",
        "  args:\n",
        "    dataset: 使用的数据集         'A'为竞赛数据集A，'B'为竞赛数据集B，'M'为MIMIC-III数据集\n",
        "    nan_alg: 处理空值使用的算法    0为填充0，1为填充平均值\n",
        "\n",
        "  note:\n",
        "    setA数据文件索引范围1~20643，19000之后有序号缺失，共计20337个病人的记录，最长时间序列长度336\n",
        "      导出矩阵尺寸为 X:(20337, 336, 40) Y:(20337, 336)\n",
        "    setB数据文件索引范围100001~120000，共计20000个病人的记录，最长时间序列长度336\n",
        "      导出矩阵尺寸为 X:(20000, 336, 40) Y:(20000, 336)\n",
        "  '''\n",
        "\n",
        "  file_name_pattern = 'p{:0=6}.psv'\n",
        "  if dataset == 'A':\n",
        "    file_index = list(range(1, 20644))\n",
        "  elif dataset == 'B':\n",
        "    file_index = list(range(100001, 102001))\n",
        "  elif dataset == 'M':\n",
        "    pass\n",
        "\n",
        "  np.random.seed(1234888)\n",
        "  np.random.shuffle(file_index)\n",
        "\n",
        "  split_ratio = np.array([0.7, 0.15, 0.15]) # train_set 0.7, valid_set 0.15, test_set 0.15\n",
        "  split_len = np.round(split_ratio * len(file_index)).astype(np.int16) \n",
        "  dataset_index = [\n",
        "    file_index[:split_len[0]],                            # train_set_index\n",
        "    file_index[split_len[0]:split_len[0] + split_len[1]], # valid_set_index\n",
        "    file_index[split_len[0] + split_len[1]:]             # test_set_index\n",
        "  ]\n",
        "  \n",
        "  datas = [\n",
        "    [np.zeros((split_len[0], 336, 40)), np.zeros((split_len[0], 336))], # train_set\n",
        "    [np.zeros((split_len[1], 336, 40)), np.zeros((split_len[1], 336))], # valid_set\n",
        "    [np.zeros((split_len[2], 336, 40)), np.zeros((split_len[2], 336))], # test_set\n",
        "  ]\n",
        "\n",
        "  # read file\n",
        "  for i in range(3):\n",
        "    for j in tqdm(dataset_index[i]):\n",
        "      file_name = file_name_pattern.format(j + 1)\n",
        "      try:\n",
        "        datafile = open(data_dir + file_name)\n",
        "        read_psv(datafile, datas[i][0][j], datas[i][1][j])\n",
        "      except:\n",
        "        pass\n",
        "      finally:\n",
        "        datafile.close()\n",
        "    datas[i][0] = nan_process(datas[i][0], nan_alg)\n",
        "    datas[i][1] = nan_process(datas[i][1], nan_alg)\n",
        "  \n",
        "  for data in datas:\n",
        "    data[1] = torch.Tensor(data[1])\n",
        "    data[0] = torch.Tensor(data[0])\n",
        "\n",
        "  return datas[0], datas[1], datas[2]\n",
        "\n",
        "print(\"data generator initialized\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data generator initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkxPZvuXoEiO"
      },
      "source": [
        "# sepsis.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    panic(\"CUDA is not available\")\n",
        "\n",
        "# ------------------ arg -------------------------\n",
        "\n",
        "dropout = 0.25\n",
        "clip = 0.2\n",
        "optim = 'Adam'\n",
        "kernel_size = 5\n",
        "log_interval = 100\n",
        "lr = 1e-3\n",
        "epochs = 100\n",
        "\n",
        "nhid = 150\n",
        "levels = 4\n",
        "input_size = 88\n",
        "seed = 6783\n",
        "model_version = 1\n",
        "\n",
        "# -----------------------------------------------\n",
        "\n",
        "# data\n",
        "train, valid, test = data_generator()\n",
        "\n",
        "# model\n",
        "model = TCN(input_size, 1, n_channels, kernel_size, dropout=_dropout)\n",
        "model.cuda()\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "n_channels = [nhid] * levels\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = getattr(torch.optim, optim)(model.parameters(), lr=lr)\n",
        "\n",
        "def evaluate(X_data, name='Eval'):\n",
        "    model.eval()\n",
        "    eval_idx_list = np.arange(len(X_data), dtype=\"int32\")\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for idx in eval_idx_list:\n",
        "            data_line = X_data[idx]\n",
        "            x, y = Variable(data_line[:-1]), Variable(data_line[1:])\n",
        "            if _cuda:\n",
        "                x, y = x.cuda(), y.cuda()\n",
        "            output = model(x.unsqueeze(0)).squeeze(0)\n",
        "            loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n",
        "                                torch.matmul((1-y), torch.log(1-output).float().t()))\n",
        "            total_loss += loss.item()\n",
        "            count += output.size(0)\n",
        "        eval_loss = total_loss / count\n",
        "        print(name + \" loss: {:.5f}\".format(eval_loss))\n",
        "        return eval_loss\n",
        "\n",
        "\n",
        "def train(ep):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    train_idx_list = np.arange(len(X_train), dtype=\"int32\")\n",
        "    np.random.shuffle(train_idx_list)\n",
        "    for idx in train_idx_list:\n",
        "        data_line = X_train[idx]\n",
        "        x, y = Variable(data_line[:-1]), Variable(data_line[1:])\n",
        "        if _cuda:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x.unsqueeze(0)).squeeze(0)\n",
        "        loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n",
        "                            torch.matmul((1 - y), torch.log(1 - output).float().t()))\n",
        "        total_loss += loss.item()\n",
        "        count += output.size(0)\n",
        "\n",
        "        if clip > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if idx > 0 and idx % log_interval == 0:\n",
        "            cur_loss = total_loss / count\n",
        "            print(\"Epoch {:2d} | lr {:.5f} | loss {:.5f}\".format(ep, lr, cur_loss))\n",
        "            total_loss = 0.0\n",
        "            count = 0\n",
        "\n",
        "best_vloss = 1e8\n",
        "vloss_list = []\n",
        "model_name = \"sepsis_predict_{0}.pt\".format(model_version)\n",
        "for ep in range(1, epochs+1):\n",
        "    train(ep)\n",
        "    vloss = evaluate(valid, name='Validation')\n",
        "    tloss = evaluate(test, name='Test')\n",
        "    if vloss < best_vloss:\n",
        "        with open(model_name, \"wb\") as f:\n",
        "            torch.save(model, f)\n",
        "            print(\"Saved model!\\n\")\n",
        "        best_vloss = vloss\n",
        "    if ep > 10 and vloss > max(vloss_list[-3:]):\n",
        "        lr /= 10\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    vloss_list.append(vloss)\n",
        "\n",
        "print('-' * 89)\n",
        "model = torch.load(open(model_name, \"rb\"))\n",
        "tloss = evaluate(test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1400 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'datafile' referenced before assignment",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-1f4013be6ca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-9-a9f25499e1e5>\u001b[0m in \u001b[0;36mdata_generator\u001b[1;34m(dataset, nan_alg)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mdatafile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mdatas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnan_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnan_alg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mdatas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnan_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnan_alg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'datafile' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.        ]\n [0.62074491 0.44682882 0.        ]]\n[[[0.         0.         0.        ]\n  [0.62074491 0.44682882 0.        ]]\n\n [[0.68417997 0.29076988 0.75393449]\n  [0.80377299 0.06026075 0.95169849]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# split_ratio = np.array([0.7, 0.15, 0.15]) # train_set 0.7, valid_set 0.15, test_set 0.15\n",
        "# split_len = np.round(split_ratio * 20001).astype(np.int16) \n",
        "\n",
        "# print(split_len[0])\n",
        "\n",
        "# print(split_ratio)\n",
        "# print(split_len)\n",
        "\n",
        "# torch.Tensor(np.random.rand(2,2))\n",
        "\n",
        "a = np.random.rand(2, 2, 3)\n",
        "b = np.zeros((2, 3))\n",
        "a0 = a[0]\n",
        "a0[1,2] = 0\n",
        "a0[0] = b[0, :]\n",
        "print(a0)\n",
        "print(a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}