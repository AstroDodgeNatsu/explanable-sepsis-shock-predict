{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poly_music.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC_dqCrdbV9I"
      },
      "source": [
        "# install env\n",
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda=9.0.176-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smvOC7ABbutN",
        "outputId": "4b48bce6-4e23-4946-b38f-4c01878477ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "# test env\n",
        "import torch\n",
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Oct 27 05:43:33 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OORqDoikBoY",
        "outputId": "0a89e8b8-a5b2-400a-8a41-5b0333107daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount google drive to env\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "# 4/5gFuY3co0BuaYE4ITNOoVXc0nXxU0r_ABpajjkpPknQqglx-VDa3yQM\n",
        "\n",
        "data_dir = \"/content/drive/My Drive/colab/data/\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fSe4TfLcoVO",
        "outputId": "1fdf383e-3f4d-496b-fec9-31810c9c6857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tcn.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "print(\"TCN component initialized\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TCN component initialized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2EAMJ83cryg",
        "outputId": "b733bca1-0d61-4472-e236-57ae5e38c4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# model.py\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
        "        super(TCN, self).__init__()\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
        "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
        "        output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
        "        output = self.linear(output).double()\n",
        "        return self.sig(output)\n",
        "\n",
        "\n",
        "print(\"TCN model initialized\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TCN model initialized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb0A_WI4cu0L",
        "outputId": "343579b9-9537-438f-e41c-f6aba2b21b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# utils.py\n",
        "from scipy.io import loadmat\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def data_generator(dataset):\n",
        "    if dataset == \"JSB\":\n",
        "        print('loading JSB data...')\n",
        "        data = loadmat(data_dir + 'mdata/JSB_Chorales.mat')\n",
        "    elif dataset == \"Muse\":\n",
        "        print('loading Muse data...')\n",
        "        data = loadmat(data_dir + 'mdata/MuseData.mat')\n",
        "    elif dataset == \"Nott\":\n",
        "        print('loading Nott data...')\n",
        "        data = loadmat(data_dir + 'mdata/Nottingham.mat')\n",
        "    elif dataset == \"Piano\":\n",
        "        print('loading Piano data...')\n",
        "        data = loadmat(data_dir + 'mdata/Piano_midi.mat')\n",
        "\n",
        "    X_train = data['traindata'][0]\n",
        "    X_valid = data['validdata'][0]\n",
        "    X_test = data['testdata'][0]\n",
        "\n",
        "    for data in [X_train, X_valid, X_test]:\n",
        "        for i in range(len(data)):\n",
        "            data[i] = torch.Tensor(data[i].astype(np.float64))\n",
        "\n",
        "    return X_train, X_valid, X_test\n",
        "\n",
        "print(\"data generator initialized\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data generator initialized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGpOHZfdFQN",
        "outputId": "224c485e-5f95-407b-9d89-9b24c0b2110f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# music_test.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "_cuda = True\n",
        "_dropout = 0.25\n",
        "_clip = 0.2\n",
        "_epochs = 100\n",
        "_ksize = 5\n",
        "_levels = 4\n",
        "_log_interval = 100\n",
        "_lr = 1e-3\n",
        "_optim = 'Adam'\n",
        "_nhid = 150\n",
        "_data = 'Nott'\n",
        "_seed = 1111\n",
        "input_size = 88\n",
        "\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(_seed)\n",
        "\n",
        "X_train, X_valid, X_test = data_generator(_data)\n",
        "\n",
        "n_channels = [_nhid] * _levels\n",
        "kernel_size = _ksize\n",
        "dropout = _dropout\n",
        "\n",
        "model = TCN(input_size, input_size, n_channels, kernel_size, dropout=_dropout)\n",
        "\n",
        "\n",
        "if _cuda:\n",
        "    model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = _lr\n",
        "optimizer = getattr(optim, _optim)(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def evaluate(X_data, name='Eval'):\n",
        "    model.eval()\n",
        "    eval_idx_list = np.arange(len(X_data), dtype=\"int32\")\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for idx in eval_idx_list:\n",
        "            data_line = X_data[idx]\n",
        "            x, y = Variable(data_line[:-1]), Variable(data_line[1:])\n",
        "            if _cuda:\n",
        "                x, y = x.cuda(), y.cuda()\n",
        "            output = model(x.unsqueeze(0)).squeeze(0)\n",
        "            loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n",
        "                                torch.matmul((1-y), torch.log(1-output).float().t()))\n",
        "            total_loss += loss.item()\n",
        "            count += output.size(0)\n",
        "        eval_loss = total_loss / count\n",
        "        print(name + \" loss: {:.5f}\".format(eval_loss))\n",
        "        return eval_loss\n",
        "\n",
        "\n",
        "def train(ep):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    train_idx_list = np.arange(len(X_train), dtype=\"int32\")\n",
        "    np.random.shuffle(train_idx_list)\n",
        "    for idx in train_idx_list:\n",
        "        data_line = X_train[idx]\n",
        "        x, y = Variable(data_line[:-1]), Variable(data_line[1:])\n",
        "        if _cuda:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x.unsqueeze(0)).squeeze(0)\n",
        "        loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n",
        "                            torch.matmul((1 - y), torch.log(1 - output).float().t()))\n",
        "        total_loss += loss.item()\n",
        "        count += output.size(0)\n",
        "\n",
        "        if _clip > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), _clip)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if idx > 0 and idx % _log_interval == 0:\n",
        "            cur_loss = total_loss / count\n",
        "            print(\"Epoch {:2d} | lr {:.5f} | loss {:.5f}\".format(ep, lr, cur_loss))\n",
        "            total_loss = 0.0\n",
        "            count = 0\n",
        "\n",
        "best_vloss = 1e8\n",
        "vloss_list = []\n",
        "model_name = \"poly_music_{0}.pt\".format(_data)\n",
        "for ep in range(1, _epochs+1):\n",
        "    train(ep)\n",
        "    vloss = evaluate(X_valid, name='Validation')\n",
        "    tloss = evaluate(X_test, name='Test')\n",
        "    if vloss < best_vloss:\n",
        "        with open(model_name, \"wb\") as f:\n",
        "            torch.save(model, f)\n",
        "            print(\"Saved model!\\n\")\n",
        "        best_vloss = vloss\n",
        "    if ep > 10 and vloss > max(vloss_list[-3:]):\n",
        "        lr /= 10\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    vloss_list.append(vloss)\n",
        "\n",
        "print('-' * 89)\n",
        "model = torch.load(open(model_name, \"rb\"))\n",
        "tloss = evaluate(X_test)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading Nott data...\n",
            "Epoch  1 | lr 0.00100 | loss 19.13266\n",
            "Epoch  1 | lr 0.00100 | loss 11.27860\n",
            "Epoch  1 | lr 0.00100 | loss 10.00171\n",
            "Epoch  1 | lr 0.00100 | loss 10.50170\n",
            "Epoch  1 | lr 0.00100 | loss 8.56560\n",
            "Epoch  1 | lr 0.00100 | loss 6.94054\n",
            "Validation loss: 5.47906\n",
            "Test loss: 5.60538\n",
            "Saved model!\n",
            "\n",
            "Epoch  2 | lr 0.00100 | loss 8.00639\n",
            "Epoch  2 | lr 0.00100 | loss 5.69376\n",
            "Epoch  2 | lr 0.00100 | loss 5.43191\n",
            "Epoch  2 | lr 0.00100 | loss 4.98212\n",
            "Epoch  2 | lr 0.00100 | loss 5.02198\n",
            "Epoch  2 | lr 0.00100 | loss 4.92638\n",
            "Validation loss: 4.61559\n",
            "Test loss: 4.66700\n",
            "Saved model!\n",
            "\n",
            "Epoch  3 | lr 0.00100 | loss 4.54105\n",
            "Epoch  3 | lr 0.00100 | loss 4.62188\n",
            "Epoch  3 | lr 0.00100 | loss 4.84626\n",
            "Epoch  3 | lr 0.00100 | loss 4.57656\n",
            "Epoch  3 | lr 0.00100 | loss 4.49996\n",
            "Epoch  3 | lr 0.00100 | loss 4.42602\n",
            "Validation loss: 4.29771\n",
            "Test loss: 4.34296\n",
            "Saved model!\n",
            "\n",
            "Epoch  4 | lr 0.00100 | loss 4.25152\n",
            "Epoch  4 | lr 0.00100 | loss 4.34381\n",
            "Epoch  4 | lr 0.00100 | loss 4.36195\n",
            "Epoch  4 | lr 0.00100 | loss 4.25044\n",
            "Epoch  4 | lr 0.00100 | loss 4.18635\n",
            "Epoch  4 | lr 0.00100 | loss 4.14105\n",
            "Validation loss: 4.03006\n",
            "Test loss: 4.04962\n",
            "Saved model!\n",
            "\n",
            "Epoch  5 | lr 0.00100 | loss 4.08584\n",
            "Epoch  5 | lr 0.00100 | loss 4.04476\n",
            "Epoch  5 | lr 0.00100 | loss 3.88255\n",
            "Epoch  5 | lr 0.00100 | loss 4.04663\n",
            "Epoch  5 | lr 0.00100 | loss 4.07661\n",
            "Epoch  5 | lr 0.00100 | loss 3.86616\n",
            "Validation loss: 3.73960\n",
            "Test loss: 3.77936\n",
            "Saved model!\n",
            "\n",
            "Epoch  6 | lr 0.00100 | loss 3.74699\n",
            "Epoch  6 | lr 0.00100 | loss 3.87055\n",
            "Epoch  6 | lr 0.00100 | loss 3.77772\n",
            "Epoch  6 | lr 0.00100 | loss 4.12868\n",
            "Epoch  6 | lr 0.00100 | loss 3.66366\n",
            "Epoch  6 | lr 0.00100 | loss 3.93222\n",
            "Validation loss: 3.62366\n",
            "Test loss: 3.64807\n",
            "Saved model!\n",
            "\n",
            "Epoch  7 | lr 0.00100 | loss 3.66820\n",
            "Epoch  7 | lr 0.00100 | loss 3.61664\n",
            "Epoch  7 | lr 0.00100 | loss 3.56132\n",
            "Epoch  7 | lr 0.00100 | loss 3.67603\n",
            "Epoch  7 | lr 0.00100 | loss 3.29324\n",
            "Epoch  7 | lr 0.00100 | loss 3.51033\n",
            "Validation loss: 3.51474\n",
            "Test loss: 3.53423\n",
            "Saved model!\n",
            "\n",
            "Epoch  8 | lr 0.00100 | loss 3.32490\n",
            "Epoch  8 | lr 0.00100 | loss 3.51181\n",
            "Epoch  8 | lr 0.00100 | loss 3.56485\n",
            "Epoch  8 | lr 0.00100 | loss 3.59149\n",
            "Epoch  8 | lr 0.00100 | loss 3.54633\n",
            "Epoch  8 | lr 0.00100 | loss 3.43814\n",
            "Validation loss: 3.43638\n",
            "Test loss: 3.45934\n",
            "Saved model!\n",
            "\n",
            "Epoch  9 | lr 0.00100 | loss 3.93969\n",
            "Epoch  9 | lr 0.00100 | loss 3.43266\n",
            "Epoch  9 | lr 0.00100 | loss 3.49776\n",
            "Epoch  9 | lr 0.00100 | loss 3.35379\n",
            "Epoch  9 | lr 0.00100 | loss 3.36025\n",
            "Epoch  9 | lr 0.00100 | loss 3.97633\n",
            "Validation loss: 3.38552\n",
            "Test loss: 3.40167\n",
            "Saved model!\n",
            "\n",
            "Epoch 10 | lr 0.00100 | loss 3.42979\n",
            "Epoch 10 | lr 0.00100 | loss 3.37326\n",
            "Epoch 10 | lr 0.00100 | loss 3.40098\n",
            "Epoch 10 | lr 0.00100 | loss 3.34742\n",
            "Epoch 10 | lr 0.00100 | loss 3.49076\n",
            "Epoch 10 | lr 0.00100 | loss 3.30028\n",
            "Validation loss: 3.37678\n",
            "Test loss: 3.37643\n",
            "Saved model!\n",
            "\n",
            "Epoch 11 | lr 0.00100 | loss 3.37994\n",
            "Epoch 11 | lr 0.00100 | loss 3.27436\n",
            "Epoch 11 | lr 0.00100 | loss 3.30074\n",
            "Epoch 11 | lr 0.00100 | loss 3.28236\n",
            "Epoch 11 | lr 0.00100 | loss 3.27028\n",
            "Epoch 11 | lr 0.00100 | loss 3.24636\n",
            "Validation loss: 3.30159\n",
            "Test loss: 3.29700\n",
            "Saved model!\n",
            "\n",
            "Epoch 12 | lr 0.00100 | loss 3.26643\n",
            "Epoch 12 | lr 0.00100 | loss 3.20325\n",
            "Epoch 12 | lr 0.00100 | loss 3.16777\n",
            "Epoch 12 | lr 0.00100 | loss 3.29592\n",
            "Epoch 12 | lr 0.00100 | loss 3.21957\n",
            "Epoch 12 | lr 0.00100 | loss 3.34398\n",
            "Validation loss: 3.24757\n",
            "Test loss: 3.25891\n",
            "Saved model!\n",
            "\n",
            "Epoch 13 | lr 0.00100 | loss 2.99419\n",
            "Epoch 13 | lr 0.00100 | loss 3.13291\n",
            "Epoch 13 | lr 0.00100 | loss 3.23652\n",
            "Epoch 13 | lr 0.00100 | loss 3.24315\n",
            "Epoch 13 | lr 0.00100 | loss 3.22136\n",
            "Epoch 13 | lr 0.00100 | loss 3.17094\n",
            "Validation loss: 3.23687\n",
            "Test loss: 3.23841\n",
            "Saved model!\n",
            "\n",
            "Epoch 14 | lr 0.00100 | loss 3.17519\n",
            "Epoch 14 | lr 0.00100 | loss 3.23798\n",
            "Epoch 14 | lr 0.00100 | loss 3.12737\n",
            "Epoch 14 | lr 0.00100 | loss 2.82132\n",
            "Epoch 14 | lr 0.00100 | loss 3.16917\n",
            "Epoch 14 | lr 0.00100 | loss 3.15965\n",
            "Validation loss: 3.19956\n",
            "Test loss: 3.20754\n",
            "Saved model!\n",
            "\n",
            "Epoch 15 | lr 0.00100 | loss 3.13565\n",
            "Epoch 15 | lr 0.00100 | loss 2.98855\n",
            "Epoch 15 | lr 0.00100 | loss 3.13223\n",
            "Epoch 15 | lr 0.00100 | loss 3.25449\n",
            "Epoch 15 | lr 0.00100 | loss 3.08750\n",
            "Epoch 15 | lr 0.00100 | loss 3.14367\n",
            "Validation loss: 3.19820\n",
            "Test loss: 3.19888\n",
            "Saved model!\n",
            "\n",
            "Epoch 16 | lr 0.00100 | loss 3.05410\n",
            "Epoch 16 | lr 0.00100 | loss 3.08499\n",
            "Epoch 16 | lr 0.00100 | loss 3.05891\n",
            "Epoch 16 | lr 0.00100 | loss 3.19455\n",
            "Epoch 16 | lr 0.00100 | loss 3.07891\n",
            "Epoch 16 | lr 0.00100 | loss 3.12258\n",
            "Validation loss: 3.16187\n",
            "Test loss: 3.16928\n",
            "Saved model!\n",
            "\n",
            "Epoch 17 | lr 0.00100 | loss 3.06229\n",
            "Epoch 17 | lr 0.00100 | loss 2.83927\n",
            "Epoch 17 | lr 0.00100 | loss 2.99815\n",
            "Epoch 17 | lr 0.00100 | loss 3.06289\n",
            "Epoch 17 | lr 0.00100 | loss 3.01300\n",
            "Epoch 17 | lr 0.00100 | loss 3.03727\n",
            "Validation loss: 3.16282\n",
            "Test loss: 3.17497\n",
            "Epoch 18 | lr 0.00100 | loss 2.80115\n",
            "Epoch 18 | lr 0.00100 | loss 2.95245\n",
            "Epoch 18 | lr 0.00100 | loss 2.91848\n",
            "Epoch 18 | lr 0.00100 | loss 3.03799\n",
            "Epoch 18 | lr 0.00100 | loss 3.03498\n",
            "Epoch 18 | lr 0.00100 | loss 2.99706\n",
            "Validation loss: 3.13236\n",
            "Test loss: 3.14839\n",
            "Saved model!\n",
            "\n",
            "Epoch 19 | lr 0.00100 | loss 3.15245\n",
            "Epoch 19 | lr 0.00100 | loss 2.90824\n",
            "Epoch 19 | lr 0.00100 | loss 2.90513\n",
            "Epoch 19 | lr 0.00100 | loss 2.96650\n",
            "Epoch 19 | lr 0.00100 | loss 2.96143\n",
            "Epoch 19 | lr 0.00100 | loss 2.95958\n",
            "Validation loss: 3.16769\n",
            "Test loss: 3.17308\n",
            "Epoch 20 | lr 0.00010 | loss 2.85872\n",
            "Epoch 20 | lr 0.00010 | loss 2.93131\n",
            "Epoch 20 | lr 0.00010 | loss 2.82361\n",
            "Epoch 20 | lr 0.00010 | loss 2.80733\n",
            "Epoch 20 | lr 0.00010 | loss 2.79612\n",
            "Epoch 20 | lr 0.00010 | loss 2.80743\n",
            "Validation loss: 3.07523\n",
            "Test loss: 3.08245\n",
            "Saved model!\n",
            "\n",
            "Epoch 21 | lr 0.00010 | loss 2.68437\n",
            "Epoch 21 | lr 0.00010 | loss 2.78786\n",
            "Epoch 21 | lr 0.00010 | loss 2.78729\n",
            "Epoch 21 | lr 0.00010 | loss 2.89459\n",
            "Epoch 21 | lr 0.00010 | loss 2.80436\n",
            "Epoch 21 | lr 0.00010 | loss 2.95111\n",
            "Validation loss: 3.06395\n",
            "Test loss: 3.07206\n",
            "Saved model!\n",
            "\n",
            "Epoch 22 | lr 0.00010 | loss 2.78652\n",
            "Epoch 22 | lr 0.00010 | loss 2.74538\n",
            "Epoch 22 | lr 0.00010 | loss 2.67240\n",
            "Epoch 22 | lr 0.00010 | loss 2.83987\n",
            "Epoch 22 | lr 0.00010 | loss 2.83443\n",
            "Epoch 22 | lr 0.00010 | loss 2.72198\n",
            "Validation loss: 3.06333\n",
            "Test loss: 3.07086\n",
            "Saved model!\n",
            "\n",
            "Epoch 23 | lr 0.00010 | loss 2.95771\n",
            "Epoch 23 | lr 0.00010 | loss 2.80176\n",
            "Epoch 23 | lr 0.00010 | loss 2.78734\n",
            "Epoch 23 | lr 0.00010 | loss 2.77474\n",
            "Epoch 23 | lr 0.00010 | loss 2.64469\n",
            "Epoch 23 | lr 0.00010 | loss 2.76986\n",
            "Validation loss: 3.06116\n",
            "Test loss: 3.06727\n",
            "Saved model!\n",
            "\n",
            "Epoch 24 | lr 0.00010 | loss 2.71396\n",
            "Epoch 24 | lr 0.00010 | loss 2.85428\n",
            "Epoch 24 | lr 0.00010 | loss 2.67314\n",
            "Epoch 24 | lr 0.00010 | loss 3.02364\n",
            "Epoch 24 | lr 0.00010 | loss 2.82024\n",
            "Epoch 24 | lr 0.00010 | loss 2.72915\n",
            "Validation loss: 3.05789\n",
            "Test loss: 3.06431\n",
            "Saved model!\n",
            "\n",
            "Epoch 25 | lr 0.00010 | loss 2.72881\n",
            "Epoch 25 | lr 0.00010 | loss 2.07216\n",
            "Epoch 25 | lr 0.00010 | loss 2.78030\n",
            "Epoch 25 | lr 0.00010 | loss 2.74755\n",
            "Epoch 25 | lr 0.00010 | loss 2.73022\n",
            "Epoch 25 | lr 0.00010 | loss 2.82055\n",
            "Validation loss: 3.05421\n",
            "Test loss: 3.06189\n",
            "Saved model!\n",
            "\n",
            "Epoch 26 | lr 0.00010 | loss 2.70394\n",
            "Epoch 26 | lr 0.00010 | loss 2.73070\n",
            "Epoch 26 | lr 0.00010 | loss 2.70751\n",
            "Epoch 26 | lr 0.00010 | loss 2.75944\n",
            "Epoch 26 | lr 0.00010 | loss 2.87378\n",
            "Epoch 26 | lr 0.00010 | loss 2.67809\n",
            "Validation loss: 3.05245\n",
            "Test loss: 3.05856\n",
            "Saved model!\n",
            "\n",
            "Epoch 27 | lr 0.00010 | loss 3.15951\n",
            "Epoch 27 | lr 0.00010 | loss 2.76466\n",
            "Epoch 27 | lr 0.00010 | loss 2.78331\n",
            "Epoch 27 | lr 0.00010 | loss 2.69665\n",
            "Epoch 27 | lr 0.00010 | loss 2.72737\n",
            "Epoch 27 | lr 0.00010 | loss 2.74985\n",
            "Validation loss: 3.04887\n",
            "Test loss: 3.05690\n",
            "Saved model!\n",
            "\n",
            "Epoch 28 | lr 0.00010 | loss 2.66239\n",
            "Epoch 28 | lr 0.00010 | loss 2.76260\n",
            "Epoch 28 | lr 0.00010 | loss 2.70218\n",
            "Epoch 28 | lr 0.00010 | loss 2.78745\n",
            "Epoch 28 | lr 0.00010 | loss 2.74557\n",
            "Epoch 28 | lr 0.00010 | loss 2.70692\n",
            "Validation loss: 3.04327\n",
            "Test loss: 3.05355\n",
            "Saved model!\n",
            "\n",
            "Epoch 29 | lr 0.00010 | loss 2.70746\n",
            "Epoch 29 | lr 0.00010 | loss 2.79079\n",
            "Epoch 29 | lr 0.00010 | loss 2.89676\n",
            "Epoch 29 | lr 0.00010 | loss 2.69021\n",
            "Epoch 29 | lr 0.00010 | loss 2.72348\n",
            "Epoch 29 | lr 0.00010 | loss 2.77932\n",
            "Validation loss: 3.04770\n",
            "Test loss: 3.05436\n",
            "Epoch 30 | lr 0.00010 | loss 2.70475\n",
            "Epoch 30 | lr 0.00010 | loss 2.67645\n",
            "Epoch 30 | lr 0.00010 | loss 2.70894\n",
            "Epoch 30 | lr 0.00010 | loss 2.63604\n",
            "Epoch 30 | lr 0.00010 | loss 2.80945\n",
            "Epoch 30 | lr 0.00010 | loss 2.58360\n",
            "Validation loss: 3.03902\n",
            "Test loss: 3.04941\n",
            "Saved model!\n",
            "\n",
            "Epoch 31 | lr 0.00010 | loss 2.67651\n",
            "Epoch 31 | lr 0.00010 | loss 2.72321\n",
            "Epoch 31 | lr 0.00010 | loss 2.66535\n",
            "Epoch 31 | lr 0.00010 | loss 2.68942\n",
            "Epoch 31 | lr 0.00010 | loss 2.81032\n",
            "Epoch 31 | lr 0.00010 | loss 2.72815\n",
            "Validation loss: 3.03815\n",
            "Test loss: 3.05085\n",
            "Saved model!\n",
            "\n",
            "Epoch 32 | lr 0.00010 | loss 2.78372\n",
            "Epoch 32 | lr 0.00010 | loss 2.73942\n",
            "Epoch 32 | lr 0.00010 | loss 2.75421\n",
            "Epoch 32 | lr 0.00010 | loss 2.70063\n",
            "Epoch 32 | lr 0.00010 | loss 2.74807\n",
            "Epoch 32 | lr 0.00010 | loss 2.70947\n",
            "Validation loss: 3.03459\n",
            "Test loss: 3.04618\n",
            "Saved model!\n",
            "\n",
            "Epoch 33 | lr 0.00010 | loss 2.63434\n",
            "Epoch 33 | lr 0.00010 | loss 2.53267\n",
            "Epoch 33 | lr 0.00010 | loss 2.75397\n",
            "Epoch 33 | lr 0.00010 | loss 2.74380\n",
            "Epoch 33 | lr 0.00010 | loss 2.74503\n",
            "Epoch 33 | lr 0.00010 | loss 2.65901\n",
            "Validation loss: 3.03096\n",
            "Test loss: 3.04324\n",
            "Saved model!\n",
            "\n",
            "Epoch 34 | lr 0.00010 | loss 2.70917\n",
            "Epoch 34 | lr 0.00010 | loss 2.57456\n",
            "Epoch 34 | lr 0.00010 | loss 2.66816\n",
            "Epoch 34 | lr 0.00010 | loss 2.11744\n",
            "Epoch 34 | lr 0.00010 | loss 2.78240\n",
            "Epoch 34 | lr 0.00010 | loss 2.73376\n",
            "Validation loss: 3.03177\n",
            "Test loss: 3.04100\n",
            "Epoch 35 | lr 0.00010 | loss 2.80183\n",
            "Epoch 35 | lr 0.00010 | loss 2.69695\n",
            "Epoch 35 | lr 0.00010 | loss 2.68823\n",
            "Epoch 35 | lr 0.00010 | loss 2.65619\n",
            "Epoch 35 | lr 0.00010 | loss 2.65148\n",
            "Epoch 35 | lr 0.00010 | loss 2.70468\n",
            "Validation loss: 3.03033\n",
            "Test loss: 3.03834\n",
            "Saved model!\n",
            "\n",
            "Epoch 36 | lr 0.00010 | loss 2.68354\n",
            "Epoch 36 | lr 0.00010 | loss 2.64936\n",
            "Epoch 36 | lr 0.00010 | loss 1.81881\n",
            "Epoch 36 | lr 0.00010 | loss 2.67335\n",
            "Epoch 36 | lr 0.00010 | loss 2.79891\n",
            "Epoch 36 | lr 0.00010 | loss 2.67806\n",
            "Validation loss: 3.02368\n",
            "Test loss: 3.03347\n",
            "Saved model!\n",
            "\n",
            "Epoch 37 | lr 0.00010 | loss 2.68205\n",
            "Epoch 37 | lr 0.00010 | loss 2.70540\n",
            "Epoch 37 | lr 0.00010 | loss 2.66066\n",
            "Epoch 37 | lr 0.00010 | loss 2.77602\n",
            "Epoch 37 | lr 0.00010 | loss 2.75114\n",
            "Epoch 37 | lr 0.00010 | loss 2.70766\n",
            "Validation loss: 3.02615\n",
            "Test loss: 3.03593\n",
            "Epoch 38 | lr 0.00010 | loss 2.61118\n",
            "Epoch 38 | lr 0.00010 | loss 2.57287\n",
            "Epoch 38 | lr 0.00010 | loss 2.45662\n",
            "Epoch 38 | lr 0.00010 | loss 2.68269\n",
            "Epoch 38 | lr 0.00010 | loss 2.69419\n",
            "Epoch 38 | lr 0.00010 | loss 2.37927\n",
            "Validation loss: 3.03102\n",
            "Test loss: 3.03990\n",
            "Epoch 39 | lr 0.00001 | loss 2.66712\n",
            "Epoch 39 | lr 0.00001 | loss 2.87557\n",
            "Epoch 39 | lr 0.00001 | loss 2.62072\n",
            "Epoch 39 | lr 0.00001 | loss 2.70059\n",
            "Epoch 39 | lr 0.00001 | loss 2.60837\n",
            "Epoch 39 | lr 0.00001 | loss 2.74563\n",
            "Validation loss: 3.02057\n",
            "Test loss: 3.03164\n",
            "Saved model!\n",
            "\n",
            "Epoch 40 | lr 0.00001 | loss 2.71398\n",
            "Epoch 40 | lr 0.00001 | loss 2.57107\n",
            "Epoch 40 | lr 0.00001 | loss 2.58859\n",
            "Epoch 40 | lr 0.00001 | loss 2.58244\n",
            "Epoch 40 | lr 0.00001 | loss 2.96027\n",
            "Epoch 40 | lr 0.00001 | loss 2.75369\n",
            "Validation loss: 3.02025\n",
            "Test loss: 3.03150\n",
            "Saved model!\n",
            "\n",
            "Epoch 41 | lr 0.00001 | loss 2.44794\n",
            "Epoch 41 | lr 0.00001 | loss 2.68153\n",
            "Epoch 41 | lr 0.00001 | loss 2.64944\n",
            "Epoch 41 | lr 0.00001 | loss 2.71172\n",
            "Epoch 41 | lr 0.00001 | loss 2.69863\n",
            "Epoch 41 | lr 0.00001 | loss 2.65171\n",
            "Validation loss: 3.01874\n",
            "Test loss: 3.03030\n",
            "Saved model!\n",
            "\n",
            "Epoch 42 | lr 0.00001 | loss 2.69531\n",
            "Epoch 42 | lr 0.00001 | loss 2.62564\n",
            "Epoch 42 | lr 0.00001 | loss 2.58342\n",
            "Epoch 42 | lr 0.00001 | loss 2.66004\n",
            "Epoch 42 | lr 0.00001 | loss 2.67142\n",
            "Epoch 42 | lr 0.00001 | loss 2.61057\n",
            "Validation loss: 3.01924\n",
            "Test loss: 3.03062\n",
            "Epoch 43 | lr 0.00001 | loss 2.57141\n",
            "Epoch 43 | lr 0.00001 | loss 2.63698\n",
            "Epoch 43 | lr 0.00001 | loss 2.64353\n",
            "Epoch 43 | lr 0.00001 | loss 2.60803\n",
            "Epoch 43 | lr 0.00001 | loss 2.68462\n",
            "Epoch 43 | lr 0.00001 | loss 2.68644\n",
            "Validation loss: 3.01879\n",
            "Test loss: 3.03020\n",
            "Epoch 44 | lr 0.00001 | loss 2.73143\n",
            "Epoch 44 | lr 0.00001 | loss 2.71957\n",
            "Epoch 44 | lr 0.00001 | loss 2.61132\n",
            "Epoch 44 | lr 0.00001 | loss 2.77147\n",
            "Epoch 44 | lr 0.00001 | loss 2.66442\n",
            "Epoch 44 | lr 0.00001 | loss 2.63990\n",
            "Validation loss: 3.01857\n",
            "Test loss: 3.03034\n",
            "Saved model!\n",
            "\n",
            "Epoch 45 | lr 0.00001 | loss 2.58447\n",
            "Epoch 45 | lr 0.00001 | loss 2.56014\n",
            "Epoch 45 | lr 0.00001 | loss 2.69797\n",
            "Epoch 45 | lr 0.00001 | loss 2.63681\n",
            "Epoch 45 | lr 0.00001 | loss 2.64898\n",
            "Epoch 45 | lr 0.00001 | loss 2.66234\n",
            "Validation loss: 3.01854\n",
            "Test loss: 3.03054\n",
            "Saved model!\n",
            "\n",
            "Epoch 46 | lr 0.00001 | loss 2.61259\n",
            "Epoch 46 | lr 0.00001 | loss 2.66215\n",
            "Epoch 46 | lr 0.00001 | loss 2.57330\n",
            "Epoch 46 | lr 0.00001 | loss 2.64194\n",
            "Epoch 46 | lr 0.00001 | loss 2.67185\n",
            "Epoch 46 | lr 0.00001 | loss 2.67954\n",
            "Validation loss: 3.01736\n",
            "Test loss: 3.02978\n",
            "Saved model!\n",
            "\n",
            "Epoch 47 | lr 0.00001 | loss 2.77754\n",
            "Epoch 47 | lr 0.00001 | loss 2.89229\n",
            "Epoch 47 | lr 0.00001 | loss 2.61858\n",
            "Epoch 47 | lr 0.00001 | loss 2.62559\n",
            "Epoch 47 | lr 0.00001 | loss 2.67586\n",
            "Epoch 47 | lr 0.00001 | loss 2.70597\n",
            "Validation loss: 3.01823\n",
            "Test loss: 3.03054\n",
            "Epoch 48 | lr 0.00001 | loss 2.57108\n",
            "Epoch 48 | lr 0.00001 | loss 2.61187\n",
            "Epoch 48 | lr 0.00001 | loss 2.66468\n",
            "Epoch 48 | lr 0.00001 | loss 2.75951\n",
            "Epoch 48 | lr 0.00001 | loss 2.70164\n",
            "Epoch 48 | lr 0.00001 | loss 2.65711\n",
            "Validation loss: 3.01840\n",
            "Test loss: 3.03057\n",
            "Epoch 49 | lr 0.00001 | loss 2.60993\n",
            "Epoch 49 | lr 0.00001 | loss 2.60975\n",
            "Epoch 49 | lr 0.00001 | loss 2.55728\n",
            "Epoch 49 | lr 0.00001 | loss 2.68195\n",
            "Epoch 49 | lr 0.00001 | loss 2.67372\n",
            "Epoch 49 | lr 0.00001 | loss 2.70604\n",
            "Validation loss: 3.01756\n",
            "Test loss: 3.02996\n",
            "Epoch 50 | lr 0.00001 | loss 2.62733\n",
            "Epoch 50 | lr 0.00001 | loss 2.53941\n",
            "Epoch 50 | lr 0.00001 | loss 2.65182\n",
            "Epoch 50 | lr 0.00001 | loss 2.66085\n",
            "Epoch 50 | lr 0.00001 | loss 2.73512\n",
            "Epoch 50 | lr 0.00001 | loss 2.62498\n",
            "Validation loss: 3.01747\n",
            "Test loss: 3.02958\n",
            "Epoch 51 | lr 0.00001 | loss 2.70193\n",
            "Epoch 51 | lr 0.00001 | loss 2.75133\n",
            "Epoch 51 | lr 0.00001 | loss 2.61939\n",
            "Epoch 51 | lr 0.00001 | loss 2.84399\n",
            "Epoch 51 | lr 0.00001 | loss 2.64788\n",
            "Epoch 51 | lr 0.00001 | loss 2.65127\n",
            "Validation loss: 3.01757\n",
            "Test loss: 3.02946\n",
            "Epoch 52 | lr 0.00001 | loss 2.62636\n",
            "Epoch 52 | lr 0.00001 | loss 2.66697\n",
            "Epoch 52 | lr 0.00001 | loss 2.68921\n",
            "Epoch 52 | lr 0.00001 | loss 2.66366\n",
            "Epoch 52 | lr 0.00001 | loss 2.61561\n",
            "Epoch 52 | lr 0.00001 | loss 2.35865\n",
            "Validation loss: 3.01698\n",
            "Test loss: 3.02876\n",
            "Saved model!\n",
            "\n",
            "Epoch 53 | lr 0.00001 | loss 2.59503\n",
            "Epoch 53 | lr 0.00001 | loss 2.63855\n",
            "Epoch 53 | lr 0.00001 | loss 2.52990\n",
            "Epoch 53 | lr 0.00001 | loss 2.72530\n",
            "Epoch 53 | lr 0.00001 | loss 2.64989\n",
            "Epoch 53 | lr 0.00001 | loss 2.63550\n",
            "Validation loss: 3.01638\n",
            "Test loss: 3.02850\n",
            "Saved model!\n",
            "\n",
            "Epoch 54 | lr 0.00001 | loss 2.81740\n",
            "Epoch 54 | lr 0.00001 | loss 2.60142\n",
            "Epoch 54 | lr 0.00001 | loss 2.40596\n",
            "Epoch 54 | lr 0.00001 | loss 2.65848\n",
            "Epoch 54 | lr 0.00001 | loss 2.63196\n",
            "Epoch 54 | lr 0.00001 | loss 2.69521\n",
            "Validation loss: 3.01685\n",
            "Test loss: 3.02880\n",
            "Epoch 55 | lr 0.00001 | loss 2.79534\n",
            "Epoch 55 | lr 0.00001 | loss 2.56324\n",
            "Epoch 55 | lr 0.00001 | loss 2.78906\n",
            "Epoch 55 | lr 0.00001 | loss 2.59992\n",
            "Epoch 55 | lr 0.00001 | loss 2.57616\n",
            "Epoch 55 | lr 0.00001 | loss 2.66007\n",
            "Validation loss: 3.01725\n",
            "Test loss: 3.02923\n",
            "Epoch 56 | lr 0.00000 | loss 2.54233\n",
            "Epoch 56 | lr 0.00000 | loss 2.62385\n",
            "Epoch 56 | lr 0.00000 | loss 2.65851\n",
            "Epoch 56 | lr 0.00000 | loss 2.71424\n",
            "Epoch 56 | lr 0.00000 | loss 2.64191\n",
            "Epoch 56 | lr 0.00000 | loss 2.74386\n",
            "Validation loss: 3.01702\n",
            "Test loss: 3.02907\n",
            "Epoch 57 | lr 0.00000 | loss 2.00412\n",
            "Epoch 57 | lr 0.00000 | loss 2.68223\n",
            "Epoch 57 | lr 0.00000 | loss 2.44169\n",
            "Epoch 57 | lr 0.00000 | loss 2.68425\n",
            "Epoch 57 | lr 0.00000 | loss 2.63956\n",
            "Epoch 57 | lr 0.00000 | loss 2.56340\n",
            "Validation loss: 3.01696\n",
            "Test loss: 3.02906\n",
            "Epoch 58 | lr 0.00000 | loss 2.65890\n",
            "Epoch 58 | lr 0.00000 | loss 2.63858\n",
            "Epoch 58 | lr 0.00000 | loss 2.58342\n",
            "Epoch 58 | lr 0.00000 | loss 3.10882\n",
            "Epoch 58 | lr 0.00000 | loss 2.67153\n",
            "Epoch 58 | lr 0.00000 | loss 2.71792\n",
            "Validation loss: 3.01668\n",
            "Test loss: 3.02890\n",
            "Epoch 59 | lr 0.00000 | loss 2.66048\n",
            "Epoch 59 | lr 0.00000 | loss 2.64848\n",
            "Epoch 59 | lr 0.00000 | loss 2.54335\n",
            "Epoch 59 | lr 0.00000 | loss 2.66126\n",
            "Epoch 59 | lr 0.00000 | loss 2.50475\n",
            "Epoch 59 | lr 0.00000 | loss 2.63249\n",
            "Validation loss: 3.01669\n",
            "Test loss: 3.02889\n",
            "Epoch 60 | lr 0.00000 | loss 2.67459\n",
            "Epoch 60 | lr 0.00000 | loss 2.65577\n",
            "Epoch 60 | lr 0.00000 | loss 2.68201\n",
            "Epoch 60 | lr 0.00000 | loss 2.66916\n",
            "Epoch 60 | lr 0.00000 | loss 2.60363\n",
            "Epoch 60 | lr 0.00000 | loss 2.59800\n",
            "Validation loss: 3.01657\n",
            "Test loss: 3.02881\n",
            "Epoch 61 | lr 0.00000 | loss 2.65330\n",
            "Epoch 61 | lr 0.00000 | loss 2.78318\n",
            "Epoch 61 | lr 0.00000 | loss 2.52923\n",
            "Epoch 61 | lr 0.00000 | loss 2.65956\n",
            "Epoch 61 | lr 0.00000 | loss 2.64999\n",
            "Epoch 61 | lr 0.00000 | loss 2.65038\n",
            "Validation loss: 3.01662\n",
            "Test loss: 3.02877\n",
            "Epoch 62 | lr 0.00000 | loss 2.67318\n",
            "Epoch 62 | lr 0.00000 | loss 2.60070\n",
            "Epoch 62 | lr 0.00000 | loss 2.63311\n",
            "Epoch 62 | lr 0.00000 | loss 2.62196\n",
            "Epoch 62 | lr 0.00000 | loss 2.56970\n",
            "Epoch 62 | lr 0.00000 | loss 2.68385\n",
            "Validation loss: 3.01673\n",
            "Test loss: 3.02883\n",
            "Epoch 63 | lr 0.00000 | loss 2.61706\n",
            "Epoch 63 | lr 0.00000 | loss 2.64980\n",
            "Epoch 63 | lr 0.00000 | loss 2.58057\n",
            "Epoch 63 | lr 0.00000 | loss 2.61634\n",
            "Epoch 63 | lr 0.00000 | loss 2.61900\n",
            "Epoch 63 | lr 0.00000 | loss 2.64129\n",
            "Validation loss: 3.01670\n",
            "Test loss: 3.02881\n",
            "Epoch 64 | lr 0.00000 | loss 2.34869\n",
            "Epoch 64 | lr 0.00000 | loss 2.59911\n",
            "Epoch 64 | lr 0.00000 | loss 2.70034\n",
            "Epoch 64 | lr 0.00000 | loss 2.60785\n",
            "Epoch 64 | lr 0.00000 | loss 2.70371\n",
            "Epoch 64 | lr 0.00000 | loss 2.66101\n",
            "Validation loss: 3.01669\n",
            "Test loss: 3.02880\n",
            "Epoch 65 | lr 0.00000 | loss 2.74296\n",
            "Epoch 65 | lr 0.00000 | loss 2.63847\n",
            "Epoch 65 | lr 0.00000 | loss 2.63912\n",
            "Epoch 65 | lr 0.00000 | loss 2.59116\n",
            "Epoch 65 | lr 0.00000 | loss 2.59827\n",
            "Epoch 65 | lr 0.00000 | loss 2.63502\n",
            "Validation loss: 3.01669\n",
            "Test loss: 3.02880\n",
            "Epoch 66 | lr 0.00000 | loss 2.49660\n",
            "Epoch 66 | lr 0.00000 | loss 2.70376\n",
            "Epoch 66 | lr 0.00000 | loss 2.54640\n",
            "Epoch 66 | lr 0.00000 | loss 2.63621\n",
            "Epoch 66 | lr 0.00000 | loss 2.59082\n",
            "Epoch 66 | lr 0.00000 | loss 2.66608\n",
            "Validation loss: 3.01669\n",
            "Test loss: 3.02879\n",
            "Epoch 67 | lr 0.00000 | loss 2.04780\n",
            "Epoch 67 | lr 0.00000 | loss 2.80412\n",
            "Epoch 67 | lr 0.00000 | loss 2.64092\n",
            "Epoch 67 | lr 0.00000 | loss 2.67639\n",
            "Epoch 67 | lr 0.00000 | loss 2.66696\n",
            "Epoch 67 | lr 0.00000 | loss 2.64873\n",
            "Validation loss: 3.01667\n",
            "Test loss: 3.02879\n",
            "Epoch 68 | lr 0.00000 | loss 2.63510\n",
            "Epoch 68 | lr 0.00000 | loss 2.70183\n",
            "Epoch 68 | lr 0.00000 | loss 2.56287\n",
            "Epoch 68 | lr 0.00000 | loss 2.68896\n",
            "Epoch 68 | lr 0.00000 | loss 2.56346\n",
            "Epoch 68 | lr 0.00000 | loss 2.80021\n",
            "Validation loss: 3.01666\n",
            "Test loss: 3.02878\n",
            "Epoch 69 | lr 0.00000 | loss 2.60727\n",
            "Epoch 69 | lr 0.00000 | loss 2.46782\n",
            "Epoch 69 | lr 0.00000 | loss 2.62052\n",
            "Epoch 69 | lr 0.00000 | loss 2.67959\n",
            "Epoch 69 | lr 0.00000 | loss 2.25519\n",
            "Epoch 69 | lr 0.00000 | loss 1.95951\n",
            "Validation loss: 3.01666\n",
            "Test loss: 3.02878\n",
            "Epoch 70 | lr 0.00000 | loss 2.57213\n",
            "Epoch 70 | lr 0.00000 | loss 2.67164\n",
            "Epoch 70 | lr 0.00000 | loss 2.75960\n",
            "Epoch 70 | lr 0.00000 | loss 2.63684\n",
            "Epoch 70 | lr 0.00000 | loss 2.58273\n",
            "Epoch 70 | lr 0.00000 | loss 2.63824\n",
            "Validation loss: 3.01664\n",
            "Test loss: 3.02876\n",
            "Epoch 71 | lr 0.00000 | loss 2.66997\n",
            "Epoch 71 | lr 0.00000 | loss 2.58801\n",
            "Epoch 71 | lr 0.00000 | loss 2.69841\n",
            "Epoch 71 | lr 0.00000 | loss 2.72859\n",
            "Epoch 71 | lr 0.00000 | loss 2.67874\n",
            "Epoch 71 | lr 0.00000 | loss 2.59750\n",
            "Validation loss: 3.01664\n",
            "Test loss: 3.02876\n",
            "Epoch 72 | lr 0.00000 | loss 2.63144\n",
            "Epoch 72 | lr 0.00000 | loss 2.72995\n",
            "Epoch 72 | lr 0.00000 | loss 2.50789\n",
            "Epoch 72 | lr 0.00000 | loss 2.66052\n",
            "Epoch 72 | lr 0.00000 | loss 2.59872\n",
            "Epoch 72 | lr 0.00000 | loss 2.60343\n",
            "Validation loss: 3.01662\n",
            "Test loss: 3.02875\n",
            "Epoch 73 | lr 0.00000 | loss 2.66768\n",
            "Epoch 73 | lr 0.00000 | loss 2.64132\n",
            "Epoch 73 | lr 0.00000 | loss 2.61058\n",
            "Epoch 73 | lr 0.00000 | loss 2.62821\n",
            "Epoch 73 | lr 0.00000 | loss 2.63100\n",
            "Epoch 73 | lr 0.00000 | loss 3.47496\n",
            "Validation loss: 3.01662\n",
            "Test loss: 3.02875\n",
            "Epoch 74 | lr 0.00000 | loss 2.58962\n",
            "Epoch 74 | lr 0.00000 | loss 2.62729\n",
            "Epoch 74 | lr 0.00000 | loss 2.59920\n",
            "Epoch 74 | lr 0.00000 | loss 2.62177\n",
            "Epoch 74 | lr 0.00000 | loss 2.73900\n",
            "Epoch 74 | lr 0.00000 | loss 2.71719\n",
            "Validation loss: 3.01663\n",
            "Test loss: 3.02876\n",
            "Epoch 75 | lr 0.00000 | loss 2.57766\n",
            "Epoch 75 | lr 0.00000 | loss 2.63660\n",
            "Epoch 75 | lr 0.00000 | loss 2.61248\n",
            "Epoch 75 | lr 0.00000 | loss 2.77857\n",
            "Epoch 75 | lr 0.00000 | loss 2.58931\n",
            "Epoch 75 | lr 0.00000 | loss 2.69793\n",
            "Validation loss: 3.01663\n",
            "Test loss: 3.02876\n",
            "Epoch 76 | lr 0.00000 | loss 2.61458\n",
            "Epoch 76 | lr 0.00000 | loss 2.60949\n",
            "Epoch 76 | lr 0.00000 | loss 2.62415\n",
            "Epoch 76 | lr 0.00000 | loss 2.46687\n",
            "Epoch 76 | lr 0.00000 | loss 2.60371\n",
            "Epoch 76 | lr 0.00000 | loss 2.89050\n",
            "Validation loss: 3.01661\n",
            "Test loss: 3.02875\n",
            "Epoch 77 | lr 0.00000 | loss 2.51815\n",
            "Epoch 77 | lr 0.00000 | loss 2.79577\n",
            "Epoch 77 | lr 0.00000 | loss 2.60294\n",
            "Epoch 77 | lr 0.00000 | loss 2.65817\n",
            "Epoch 77 | lr 0.00000 | loss 2.64702\n",
            "Epoch 77 | lr 0.00000 | loss 2.66886\n",
            "Validation loss: 3.01660\n",
            "Test loss: 3.02874\n",
            "Epoch 78 | lr 0.00000 | loss 2.68468\n",
            "Epoch 78 | lr 0.00000 | loss 2.68661\n",
            "Epoch 78 | lr 0.00000 | loss 2.62608\n",
            "Epoch 78 | lr 0.00000 | loss 2.63487\n",
            "Epoch 78 | lr 0.00000 | loss 2.66246\n",
            "Epoch 78 | lr 0.00000 | loss 2.57256\n",
            "Validation loss: 3.01659\n",
            "Test loss: 3.02873\n",
            "Epoch 79 | lr 0.00000 | loss 2.80047\n",
            "Epoch 79 | lr 0.00000 | loss 2.55288\n",
            "Epoch 79 | lr 0.00000 | loss 2.05825\n",
            "Epoch 79 | lr 0.00000 | loss 2.65761\n",
            "Epoch 79 | lr 0.00000 | loss 2.63492\n",
            "Epoch 79 | lr 0.00000 | loss 2.61505\n",
            "Validation loss: 3.01658\n",
            "Test loss: 3.02872\n",
            "Epoch 80 | lr 0.00000 | loss 2.58895\n",
            "Epoch 80 | lr 0.00000 | loss 2.58530\n",
            "Epoch 80 | lr 0.00000 | loss 2.61734\n",
            "Epoch 80 | lr 0.00000 | loss 2.72553\n",
            "Epoch 80 | lr 0.00000 | loss 2.56140\n",
            "Epoch 80 | lr 0.00000 | loss 2.65990\n",
            "Validation loss: 3.01657\n",
            "Test loss: 3.02871\n",
            "Epoch 81 | lr 0.00000 | loss 2.64029\n",
            "Epoch 81 | lr 0.00000 | loss 2.65920\n",
            "Epoch 81 | lr 0.00000 | loss 2.63606\n",
            "Epoch 81 | lr 0.00000 | loss 2.52870\n",
            "Epoch 81 | lr 0.00000 | loss 2.64641\n",
            "Epoch 81 | lr 0.00000 | loss 2.72579\n",
            "Validation loss: 3.01657\n",
            "Test loss: 3.02871\n",
            "Epoch 82 | lr 0.00000 | loss 2.60080\n",
            "Epoch 82 | lr 0.00000 | loss 2.59142\n",
            "Epoch 82 | lr 0.00000 | loss 3.32525\n",
            "Epoch 82 | lr 0.00000 | loss 2.63218\n",
            "Epoch 82 | lr 0.00000 | loss 2.72951\n",
            "Epoch 82 | lr 0.00000 | loss 2.65485\n",
            "Validation loss: 3.01656\n",
            "Test loss: 3.02871\n",
            "Epoch 83 | lr 0.00000 | loss 2.68443\n",
            "Epoch 83 | lr 0.00000 | loss 2.33240\n",
            "Epoch 83 | lr 0.00000 | loss 2.68437\n",
            "Epoch 83 | lr 0.00000 | loss 2.66925\n",
            "Epoch 83 | lr 0.00000 | loss 2.63007\n",
            "Epoch 83 | lr 0.00000 | loss 2.57253\n",
            "Validation loss: 3.01655\n",
            "Test loss: 3.02869\n",
            "Epoch 84 | lr 0.00000 | loss 2.45053\n",
            "Epoch 84 | lr 0.00000 | loss 2.60939\n",
            "Epoch 84 | lr 0.00000 | loss 2.49081\n",
            "Epoch 84 | lr 0.00000 | loss 2.64985\n",
            "Epoch 84 | lr 0.00000 | loss 2.65317\n",
            "Epoch 84 | lr 0.00000 | loss 2.66700\n",
            "Validation loss: 3.01654\n",
            "Test loss: 3.02869\n",
            "Epoch 85 | lr 0.00000 | loss 2.66615\n",
            "Epoch 85 | lr 0.00000 | loss 2.56067\n",
            "Epoch 85 | lr 0.00000 | loss 2.63169\n",
            "Epoch 85 | lr 0.00000 | loss 2.62910\n",
            "Epoch 85 | lr 0.00000 | loss 2.62446\n",
            "Epoch 85 | lr 0.00000 | loss 2.65356\n",
            "Validation loss: 3.01652\n",
            "Test loss: 3.02868\n",
            "Epoch 86 | lr 0.00000 | loss 2.75566\n",
            "Epoch 86 | lr 0.00000 | loss 2.60358\n",
            "Epoch 86 | lr 0.00000 | loss 2.64058\n",
            "Epoch 86 | lr 0.00000 | loss 2.73889\n",
            "Epoch 86 | lr 0.00000 | loss 2.67555\n",
            "Epoch 86 | lr 0.00000 | loss 2.64315\n",
            "Validation loss: 3.01651\n",
            "Test loss: 3.02866\n",
            "Epoch 87 | lr 0.00000 | loss 2.54989\n",
            "Epoch 87 | lr 0.00000 | loss 2.59764\n",
            "Epoch 87 | lr 0.00000 | loss 2.65650\n",
            "Epoch 87 | lr 0.00000 | loss 2.66511\n",
            "Epoch 87 | lr 0.00000 | loss 2.69696\n",
            "Epoch 87 | lr 0.00000 | loss 2.56818\n",
            "Validation loss: 3.01650\n",
            "Test loss: 3.02866\n",
            "Epoch 88 | lr 0.00000 | loss 2.54327\n",
            "Epoch 88 | lr 0.00000 | loss 2.61744\n",
            "Epoch 88 | lr 0.00000 | loss 2.64585\n",
            "Epoch 88 | lr 0.00000 | loss 2.68227\n",
            "Epoch 88 | lr 0.00000 | loss 2.82125\n",
            "Epoch 88 | lr 0.00000 | loss 2.49909\n",
            "Validation loss: 3.01651\n",
            "Test loss: 3.02866\n",
            "Epoch 89 | lr 0.00000 | loss 2.61564\n",
            "Epoch 89 | lr 0.00000 | loss 2.57357\n",
            "Epoch 89 | lr 0.00000 | loss 2.66335\n",
            "Epoch 89 | lr 0.00000 | loss 2.61687\n",
            "Epoch 89 | lr 0.00000 | loss 2.62019\n",
            "Epoch 89 | lr 0.00000 | loss 2.72427\n",
            "Validation loss: 3.01650\n",
            "Test loss: 3.02866\n",
            "Epoch 90 | lr 0.00000 | loss 2.63399\n",
            "Epoch 90 | lr 0.00000 | loss 2.71998\n",
            "Epoch 90 | lr 0.00000 | loss 2.86929\n",
            "Epoch 90 | lr 0.00000 | loss 2.59568\n",
            "Epoch 90 | lr 0.00000 | loss 2.65990\n",
            "Epoch 90 | lr 0.00000 | loss 2.60834\n",
            "Validation loss: 3.01648\n",
            "Test loss: 3.02864\n",
            "Epoch 91 | lr 0.00000 | loss 2.63884\n",
            "Epoch 91 | lr 0.00000 | loss 2.80502\n",
            "Epoch 91 | lr 0.00000 | loss 2.68388\n",
            "Epoch 91 | lr 0.00000 | loss 2.59734\n",
            "Epoch 91 | lr 0.00000 | loss 2.57156\n",
            "Epoch 91 | lr 0.00000 | loss 2.63680\n",
            "Validation loss: 3.01648\n",
            "Test loss: 3.02863\n",
            "Epoch 92 | lr 0.00000 | loss 2.67919\n",
            "Epoch 92 | lr 0.00000 | loss 2.70843\n",
            "Epoch 92 | lr 0.00000 | loss 2.63574\n",
            "Epoch 92 | lr 0.00000 | loss 2.68288\n",
            "Epoch 92 | lr 0.00000 | loss 2.59269\n",
            "Epoch 92 | lr 0.00000 | loss 2.70758\n",
            "Validation loss: 3.01648\n",
            "Test loss: 3.02863\n",
            "Epoch 93 | lr 0.00000 | loss 2.62566\n",
            "Epoch 93 | lr 0.00000 | loss 2.57146\n",
            "Epoch 93 | lr 0.00000 | loss 2.60693\n",
            "Epoch 93 | lr 0.00000 | loss 2.69155\n",
            "Epoch 93 | lr 0.00000 | loss 2.62012\n",
            "Epoch 93 | lr 0.00000 | loss 2.67480\n",
            "Validation loss: 3.01645\n",
            "Test loss: 3.02861\n",
            "Epoch 94 | lr 0.00000 | loss 2.45658\n",
            "Epoch 94 | lr 0.00000 | loss 2.63821\n",
            "Epoch 94 | lr 0.00000 | loss 2.67031\n",
            "Epoch 94 | lr 0.00000 | loss 2.77997\n",
            "Epoch 94 | lr 0.00000 | loss 2.64015\n",
            "Epoch 94 | lr 0.00000 | loss 2.74069\n",
            "Validation loss: 3.01644\n",
            "Test loss: 3.02860\n",
            "Epoch 95 | lr 0.00000 | loss 2.66660\n",
            "Epoch 95 | lr 0.00000 | loss 2.46924\n",
            "Epoch 95 | lr 0.00000 | loss 2.68399\n",
            "Epoch 95 | lr 0.00000 | loss 2.68012\n",
            "Epoch 95 | lr 0.00000 | loss 2.65047\n",
            "Epoch 95 | lr 0.00000 | loss 2.64775\n",
            "Validation loss: 3.01643\n",
            "Test loss: 3.02859\n",
            "Epoch 96 | lr 0.00000 | loss 2.54555\n",
            "Epoch 96 | lr 0.00000 | loss 2.61167\n",
            "Epoch 96 | lr 0.00000 | loss 2.67093\n",
            "Epoch 96 | lr 0.00000 | loss 2.63311\n",
            "Epoch 96 | lr 0.00000 | loss 2.59886\n",
            "Epoch 96 | lr 0.00000 | loss 2.49318\n",
            "Validation loss: 3.01642\n",
            "Test loss: 3.02857\n",
            "Epoch 97 | lr 0.00000 | loss 2.71967\n",
            "Epoch 97 | lr 0.00000 | loss 2.67433\n",
            "Epoch 97 | lr 0.00000 | loss 2.66526\n",
            "Epoch 97 | lr 0.00000 | loss 2.61725\n",
            "Epoch 97 | lr 0.00000 | loss 2.61376\n",
            "Epoch 97 | lr 0.00000 | loss 2.56189\n",
            "Validation loss: 3.01641\n",
            "Test loss: 3.02857\n",
            "Epoch 98 | lr 0.00000 | loss 2.62420\n",
            "Epoch 98 | lr 0.00000 | loss 2.67012\n",
            "Epoch 98 | lr 0.00000 | loss 2.75784\n",
            "Epoch 98 | lr 0.00000 | loss 2.44291\n",
            "Epoch 98 | lr 0.00000 | loss 2.64280\n",
            "Epoch 98 | lr 0.00000 | loss 2.64000\n",
            "Validation loss: 3.01641\n",
            "Test loss: 3.02857\n",
            "Epoch 99 | lr 0.00000 | loss 2.59509\n",
            "Epoch 99 | lr 0.00000 | loss 2.76975\n",
            "Epoch 99 | lr 0.00000 | loss 2.62024\n",
            "Epoch 99 | lr 0.00000 | loss 2.58961\n",
            "Epoch 99 | lr 0.00000 | loss 2.69630\n",
            "Epoch 99 | lr 0.00000 | loss 2.70599\n",
            "Validation loss: 3.01639\n",
            "Test loss: 3.02857\n",
            "Epoch 100 | lr 0.00000 | loss 2.63296\n",
            "Epoch 100 | lr 0.00000 | loss 2.62920\n",
            "Epoch 100 | lr 0.00000 | loss 2.64816\n",
            "Epoch 100 | lr 0.00000 | loss 2.66336\n",
            "Epoch 100 | lr 0.00000 | loss 2.61445\n",
            "Epoch 100 | lr 0.00000 | loss 2.67950\n",
            "Validation loss: 3.01638\n",
            "Test loss: 3.02856\n",
            "-----------------------------------------------------------------------------------------\n",
            "Eval loss: 3.02850\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}